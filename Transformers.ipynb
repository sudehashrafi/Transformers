{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "unlike traditional sequence models such as RNNs, transformers leverage self attention mechanisms to process and put data in parallel.\n",
        "\n",
        "transformer model consists of two main parts:\n",
        "1. the encoder\n",
        "2. the decoder\n",
        "\n",
        "\n",
        "both the encoder and the decoder are composed of layers that include self attention mechanisms and feed forward neural networks. Self-attention allows the model to weigh the importance of different words in a sentence when encoding a particular word. This is crucial for capturing dependencies that are far apart in the input sequence.\n",
        "\n",
        "The feed forward neural network layers help in transforming the input data after the self attention mechanism. Each layer in the encoder and decoder stacks multiple such sub layers enabling the model to learn complex representations."
      ],
      "metadata": {
        "id": "bou4Bvjy3tTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-attention\n",
        "\n",
        "is the core component of the transformer architecture.\n",
        "\n",
        "It allows each word and the input to attend to every other word, making it possible to capture contexts and relationships more effectively.\n",
        "\n",
        "In self-attention, each word is represented by three vectors:\n",
        "1. Query\n",
        "2. key\n",
        "3. value.\n",
        "\n",
        "The attention score is computed as a dot product of the query and key vectors, which is then used to weigh the value vectors. This process allows the model to focus on different parts of the input sequence when making predictions."
      ],
      "metadata": {
        "id": "QDp6A2xw55Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the Transformer Encoder:\n",
        "\n",
        "composed of:\n",
        "1. multiple layers\n",
        "2. self-attention mechanism\n",
        "3. feedforward neural network\n",
        "4. residual connection\n",
        "5. layer normalization\n",
        "6. Input embedded and passed through positional encoding for order of words"
      ],
      "metadata": {
        "id": "hew9M9VE8O_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5sjByQ393niW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(Layer):\n",
        "    def __init__(self, d_model):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
        "    def call(self, inputs):\n",
        "        q = self.query_dense(inputs)\n",
        "        k = self.key_dense(inputs)\n",
        "        v = self.value_dense(inputs)\n",
        "\n",
        "        attention_weights = tf.nn.softmax(tf.matmul(q,k,transpose_b=True) / tf.math.sqrt(tf.cast(self.d_model, tf.float32)) , axis=-1)\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "        return output"
      ],
      "metadata": {
        "id": "SjjzDyGJ3sNf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.random.uniform((1,60,512))\n",
        "self_attention = SelfAttention(d_model=512)\n",
        "output = self_attention(inputs)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWqXg5mP3sQW",
        "outputId": "ace69866-3731-4bc2-c601-4fc7f4ee7da6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 60, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    attn_output = self.mha(x, x, x, attention_mask = mask) # self attention\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output) # residual connection and normalization\n",
        "\n",
        "    ffn_output = self.ffn(out1) # Feed forward network\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output) # residual connection and normalization\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "yJutMx833sTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoder(d_model=512, num_heads=8, dff=2048)\n",
        "x = tf.random.uniform((1, 60, 512))\n",
        "mask = None\n",
        "output = encoder(inputs, training=True, mask=None)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "0ESxrP783sVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transofer Decoder\n",
        "\n",
        "similar to the encoder but with an additional cross attention mechanism to attend to the encoders output\n",
        "this allows the decoder to generate sequences based on the context provided by the encoder\n",
        "\n",
        "the decoder takes the target sequence as input, applies self attention and cross attention with the encoder output and then passes through a feed forward neural network"
      ],
      "metadata": {
        "id": "fJi0blKo-om-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "\n",
        "    self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "    self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    attn1 = self.mha1(x, x, x, attention_mask=look_ahead_mask) # self attention\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x) # reisudal connection and normalization\n",
        "\n",
        "    attn2 = self.mha2(enc_output, enc_output, out1, attention_mask=padding_mask) # cross attention\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1) # residual connection and normalization\n",
        "\n",
        "    ffn_output = self.ffn(out2) # feed forward network\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2) # residual connection and normalization\n",
        "\n",
        "    return out3"
      ],
      "metadata": {
        "id": "JlC1bhJ53sXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDZqXtk53saP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmiRsNmM3scz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qKUez2i3sfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFLb5dGO3shi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q4xXd6ol3sl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}